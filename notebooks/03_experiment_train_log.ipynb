{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "679196a2",
      "metadata": {},
      "source": [
        "# 03 - モデル登録と学習、MLflow 実験記録\n",
        "\n",
        "このノートブックでは、未学習の PointNet モデルを MLflow レジストリに登録し、\n",
        "その後登録済みモデルをロードして SemanticKITTI データでファインチューニングを行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5a977d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import importlib.util\n",
        "import importlib\n",
        "\n",
        "# setuptools の _distutils が使われるように強制する\n",
        "os.environ[\"SETUPTOOLS_USE_DISTUTILS\"] = \"local\"\n",
        "\n",
        "# setuptools を先にインポート\n",
        "import setuptools\n",
        "\n",
        "# 既存のdistutilsをクリア\n",
        "if \"distutils\" in sys.modules:\n",
        "    del sys.modules[\"distutils\"]\n",
        "    \n",
        "# _distutils_hackのdo_override関数をバックアップし、正しく動作するように修正\n",
        "if \"_distutils_hack\" in sys.modules:\n",
        "    original_do_override = sys.modules[\"_distutils_hack\"].do_override\n",
        "    \n",
        "    def patched_do_override():\n",
        "        try:\n",
        "            return original_do_override()\n",
        "        except AssertionError:\n",
        "            # distutilsのパスを調整\n",
        "            import distutils\n",
        "            distutils.__path__.insert(0, setuptools.__path__[0] + \"/_distutils\")\n",
        "            return True\n",
        "            \n",
        "    sys.modules[\"_distutils_hack\"].do_override = patched_do_override"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf17e30",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# プロジェクトルートをパスへ追加\n",
        "sys.path.append(os.path.abspath('..'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee7b4f5",
      "metadata": {},
      "source": [
        "## 1. ベースモデルの登録"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b53bcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from ml.models.pointnet import PointNetSeg\n",
        "\n",
        "# 実験設定\n",
        "mlflow.set_experiment('PointNetBaseRegistration')\n",
        "with mlflow.start_run(run_name='register-base') as run:\n",
        "    # 未学習モデルインスタンス化\n",
        "    base_model = PointNetSeg(num_classes=90)\n",
        "    # MLflowにモデルをログ\n",
        "    mlflow.pytorch.log_model(base_model, 'base_model')\n",
        "    # レジストリ登録\n",
        "    model_uri = f'runs:/{run.info.run_id}/base_model'\n",
        "    registered = mlflow.register_model(model_uri=model_uri, name='PointNetBase')\n",
        "    print(f'Registered model: {registered.name} v{registered.version}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a96c596",
      "metadata": {},
      "source": [
        "## 2. モデルのロード & ファインチューニング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63eec66e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.metrics import accuracy_score\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from ml.train.train_pointnet import SemanticKittiDataset\n",
        "\n",
        "# 点群データでは、サンプルごとに点の数が異なるのが一般的。\n",
        "# 一方、PyTorchのデフォルトのcollate関数は、すべてのテンソルが同じサイズであることを期待する。\n",
        "# よって、今回はカスタムcollate関数を定義\n",
        "def pointcloud_collate(batch):\n",
        "    \"\"\"点群データ用のカスタムcollate関数。異なるサイズの点群を処理します。\"\"\"\n",
        "    # バッチから点群とラベルを抽出\n",
        "    points = [item[0] for item in batch]  # points: list of [3, N_i]\n",
        "    labels = [item[1] for item in batch]  # labels: list of [N_i]\n",
        "    \n",
        "    # すべての点群から均一にサンプリングする点の数を決定\n",
        "    # 最小の点数を使用するか、固定値を使用\n",
        "    min_points = min([p.shape[1] for p in points])\n",
        "    target_points = min(min_points, 10000)  # 10000点を最大とする\n",
        "    \n",
        "    # 各点群から均一にサンプリング\n",
        "    sampled_points = []\n",
        "    sampled_labels = []\n",
        "    \n",
        "    for pts, lbls in zip(points, labels):\n",
        "        if pts.shape[1] > target_points:\n",
        "            # インデックスをランダムにサンプリング\n",
        "            idx = torch.randperm(pts.shape[1])[:target_points]\n",
        "            sampled_points.append(pts[:, idx])\n",
        "            sampled_labels.append(lbls[idx])\n",
        "        else:\n",
        "            # 点が少ない場合はそのまま使用\n",
        "            sampled_points.append(pts)\n",
        "            sampled_labels.append(lbls)\n",
        "    \n",
        "    # テンソルをスタック\n",
        "    points_batch = torch.stack(sampled_points)\n",
        "    labels_batch = torch.stack(sampled_labels)\n",
        "    \n",
        "    return points_batch, labels_batch\n",
        "\n",
        "# 実験設定\n",
        "mlflow.set_experiment('PointNetFineTune')\n",
        "with mlflow.start_run(run_name='finetune-run') as run:\n",
        "    # ベースモデルのロード\n",
        "    model = mlflow.pytorch.load_model('models:/PointNetBase/1')\n",
        "    model.train()\n",
        "\n",
        "    # データローダ作成\n",
        "    dataset = SemanticKittiDataset('data/preprocessed')\n",
        "    loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=pointcloud_collate)\n",
        "\n",
        "    # オプティマイザ・損失関数\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "    epochs = 5\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for points, labels in loader:\n",
        "            # [B,3,N] と [B,N]\n",
        "            targets = labels[:,0]\n",
        "            outputs = model(points)\n",
        "            loss = criterion(outputs, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            all_preds.extend(outputs.argmax(dim=1).tolist())\n",
        "            all_targets.extend(targets.tolist())\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        mlflow.log_metric('epoch_loss', avg_loss, step=epoch)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, loss={avg_loss:.4f}')\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    mlflow.log_metric('final_accuracy', acc)\n",
        "    print(f'Final Accuracy: {acc:.4f}')\n",
        "\n",
        "    # ファインチューニング済みモデルをログ & 登録\n",
        "    mlflow.pytorch.log_model(model, 'finetuned_pointnet')\n",
        "    mlflow.register_model(f'runs:/{run.info.run_id}/finetuned_pointnet', 'PointNetFineTuned')\n",
        "    print('Fine-tuned model registered as PointNetFineTuned')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
